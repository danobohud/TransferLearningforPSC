# LOAD LIBRARIES AND FUNCTIONSimport os,cv2import numpy as npimport Sourcingimport timefrom zipfile import BadZipFileimport tensorflow as tfstrategy = tf.distribute.MirroredStrategy()print('Number of devices: {}'.format(strategy.num_replicas_in_sync))from tensorflow.compat.v1 import ConfigProtofrom tensorflow.compat.v1 import InteractiveSessionconfig = ConfigProto(log_device_placement=False)config.gpu_options.allow_growth = Truesess = InteractiveSession(config=config)def preprocess(files,inpath,size=False):    output=[]    if not size:        size=255    f=0    for i,file in enumerate(files):        if i%1000==0:            print('%s/%s'%(i,len(files)))        try:            upload=np.load(os.path.join(inpath,file+'.npz'))            dist,anm,nb=[upload['img'][:,:,x] for x in range(3)]            dist = cv2.resize(dist,dsize=(size,size),interpolation=cv2.INTER_CUBIC)            dist =((np.clip(dist,0,50)/50)*100).astype('int')            anm=(np.clip(anm,-1,1)*100).astype('float32')            anm = cv2.resize(anm,dsize=(size,size),interpolation=cv2.INTER_CUBIC).astype('int')            nb=(cv2.resize(nb.astype('float32'),dsize=(size,size),interpolation=cv2.INTER_CUBIC)*1000).astype('int')            img=np.stack([dist,anm,nb],axis=-1)            output.append(img)        except BadZipFile:            print('Corrupt file: ', file)            f+=1            continue        except FileNotFoundError:            print(file,' not found')            f+=1            continue        except OSError:            print('OSError')            f+=1            continue    print('Completed with %s files missing'%(f))    return np.array(output)#%% PREPARE DIRECTORIESinput_folder='CATH/PQR/PQR'ID=time.strftime('%Y%m%d%H%M%S')pwd=os.getcwd()if not 'outputs' in [file for file in os.listdir(pwd)]:    os.mkdir('outputs')parent='outputs/%s'%(ID)raw,pre = [parent+'/%s'%(x) for x in ['raw','_preprocessed.npz']]for item in [parent,raw]:    os.mkdir(item)     #%% TRANSFORM IMAGES    atom_selection='ca'imgs = [file for file in os.listdir(raw) if file[-4:] == '.npz']pqrs = [file for file in os.listdir(input_folder) if file[-4:] == '.pqr' and file not in imgs][:10]print('Getting representations from %s PQR files and saving to %s' % (len(pqrs), raw))Sourcing.get_rawfiles(pqrs,input_folder, raw, atom_selection)files = [file[:-4] for file in os.listdir(raw) if file[-4:] == '.npz']print('Preprocessing %s files and saving to %s' % (len(files), pre))transform=preprocess(files,raw,255)#%%dictionary={}dictionary['X_test']=transform#%% LOAD MODELclass BaseLine(object):    '''Model object: Initialise, build, train and/or evaluate model. Adapted from        https://www.pyimagesearch.com/2018/06/04/keras-multiple-outputs-and-multiple-losses/ and        https://www.tensorflow.org/guide/keras/train_and_evaluate'''    def __init__(self, inputs,nums):        '''Set model parameters from input specifications'''        self.preprocessed_data = inputs['X_test']        self.batch_size = 32        self.nums=nums    def build(self, loadfile,output_layer=False):        '''Build multi-task DenseNet121 model.'''        strategy = tf.distribute.MirroredStrategy()                                         # Set distributed strategy        print('Number of devices: {}'.format(strategy.num_replicas_in_sync))        (numC, numA, numT, numH) = self.nums                                                # Get number of classes        with strategy.scope():            base_model = DenseNet121(weights='imagenet', include_top=True,                      # Import DenseNet121                                     input_tensor=Input(shape=(self.scaleDim, self.scaleDim, 3)))            base_model.layers.pop()                                                             # Remove the last layer            print('Base model: {}\nNumber of Layers: {}'.format(base_model.name,                                                                    len(base_model.layers)))            self.name = base_model.name            if output_layer:                self.output_layer = output_layer        # Set width of PFP layer            else:                self.output_layer = 512            # Amend DenseNet backbone for multi-task learning            x = base_model.output            PFP = Dense(self.output_layer, input_shape=(2048,), activation='relu', name='PFP')(x)   # Protein fingerprint layer            BN = BatchNormalization(name='BatchNorm')(PFP)            Drop = Dropout(0.25)(BN)            C_labels = Dense(numC, input_shape=(self.output_layer,), activation='softmax', name='C_labels')(Drop)            A_labels = Dense(numA, input_shape=(self.output_layer,), activation='softmax', name='A_labels')(Drop)            T_labels = Dense(numT, input_shape=(self.output_layer,), activation='softmax', name='T_labels')(Drop)            H_labels = Dense(numH, input_shape=(self.output_layer,), activation='softmax', name='H_labels')(Drop)            # Compile            self.model = Model(inputs=base_model.input, outputs=[C_labels, A_labels, T_labels, H_labels])            if not learning_rate:                lr = 0.001            else:                lr = learning_rate            if loadfile:            # Load weights from pre-trained model                print('Loading model weights from ', loadfile)                self.model.load_weights(loadfile)                print('Loaded')            lossWeights = {"C_labels": 1.0, "A_labels": 1.0, "T_labels": 1.0, "H_labels": 5.0}            opt = Adam(learning_rate=lr)            self.model.compile(optimizer=opt,                               loss={"C_labels": 'categorical_crossentropy', "A_labels": 'categorical_crossentropy',                                     "T_labels": 'categorical_crossentropy', "H_labels": 'categorical_crossentropy'},                               loss_weights=lossWeights,                               metrics=['accuracy'])